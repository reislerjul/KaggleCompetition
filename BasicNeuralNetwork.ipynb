{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tflearn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "import sklearn.linear_model as model\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Michelle's method for importing data since I can't do it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX =  (20000, 1000)\n",
      "trainY =  (20000,)\n",
      "testX =  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('Data/training_data.txt',dtype='str')\n",
    "test_data = np.genfromtxt('Data/test_data.txt',dtype='str')\n",
    "#train_data = np.loadtxt('training_data.txt',  delimiter=' ')\n",
    "#test_data = np.loadtxt('test_data.txt')\n",
    "train_labels = train_data[0, :]\n",
    "train_stars = train_data[1:, 0]\n",
    "train_reviews = train_data[1:, 1:]\n",
    "\n",
    "test_labels = test_data[0, :]\n",
    "#test_stars = train_data[1:, 0]\n",
    "test_reviews = test_data[1:, 0:]\n",
    "\n",
    "trainX = train_reviews\n",
    "trainY = train_stars\n",
    "testX = test_reviews\n",
    "#testY = test_stars\n",
    "\n",
    "print(\"trainX = \", trainX.shape)\n",
    "print(\"trainY = \", trainY.shape)\n",
    "print(\"testX = \", testX.shape)\n",
    "\n",
    "\n",
    "for i in range(0, len(trainX)):\n",
    "    for j in range(0, len(trainX[0])):\n",
    "        trainX[i][j] = int(trainX[i][j])\n",
    "\n",
    "for i in range(0, len(testX)):\n",
    "    for j in range(0, len(testX[0])):\n",
    "        testX[i][j] = int(testX[i][j])\n",
    "\n",
    "for i in range(0, len(trainY)):\n",
    "        trainY[i] = int(trainY[i])\n",
    "#for i in range(0, len(trainY)):\n",
    "        #testY[i] = int(testY[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for classification error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_error(y, y_pred):\n",
    "    misclassified = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            misclassified += 1\n",
    "    return float(misclassified) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into 80% training set and 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rows = []\n",
    "for i in range(len(trainY)):\n",
    "    num_rows.append(i)\n",
    "    \n",
    "np.random.shuffle(num_rows)\n",
    "train_indices = num_rows[:16000]\n",
    "valid_indices = num_rows[16000:]\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "valid_X = []\n",
    "valid_y = []\n",
    "\n",
    "for index in train_indices:\n",
    "    train_X.append(trainX[index])\n",
    "    train_y.append(trainY[index])\n",
    "    \n",
    "for index in valid_indices:\n",
    "    valid_X.append(trainX[index])\n",
    "    valid_y.append(trainY[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Michelle's code, it doesn't actually convert the array contents into integers because she tried to do it in place. This code does it correctly out of place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X2 = []\n",
    "for row in train_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    train_X2.append(temp)\n",
    "\n",
    "valid_X2 = []\n",
    "for row in valid_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    valid_X2.append(temp)\n",
    "    \n",
    "valid_y2 = []\n",
    "for row in valid_y:\n",
    "    valid_y2.append(int(row))\n",
    "    \n",
    "train_y2 = []\n",
    "for row in train_y:\n",
    "    train_y2.append(int(row))\n",
    "    \n",
    "test_X2 = []\n",
    "for row in testX:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    test_X2.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything back to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.array(train_X2)\n",
    "validX = np.array(valid_X2)\n",
    "trainY = np.array(train_y2)\n",
    "validY = np.array(valid_y2)\n",
    "testX = np.array(test_X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ..., 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "validY = to_categorical(validY, 2)\n",
    "print(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 60)                60060     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 122       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 63,842\n",
      "Trainable params: 63,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michellezhao/anaconda3/lib/python3.6/site-packages/keras/models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/35\n",
      "16000/16000 [==============================] - 1s 72us/step - loss: 0.4328 - acc: 0.8034 - val_loss: 0.3663 - val_acc: 0.8420\n",
      "Epoch 2/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3526 - acc: 0.8520 - val_loss: 0.3622 - val_acc: 0.8440\n",
      "Epoch 3/35\n",
      "16000/16000 [==============================] - 1s 37us/step - loss: 0.3376 - acc: 0.8581 - val_loss: 0.3598 - val_acc: 0.8427\n",
      "Epoch 4/35\n",
      "16000/16000 [==============================] - 1s 37us/step - loss: 0.3342 - acc: 0.8553 - val_loss: 0.3570 - val_acc: 0.8462\n",
      "Epoch 5/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3299 - acc: 0.8620 - val_loss: 0.3596 - val_acc: 0.8455\n",
      "Epoch 6/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3286 - acc: 0.8617 - val_loss: 0.3587 - val_acc: 0.8440\n",
      "Epoch 7/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3236 - acc: 0.8656 - val_loss: 0.3620 - val_acc: 0.8445\n",
      "Epoch 8/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3218 - acc: 0.8666 - val_loss: 0.3599 - val_acc: 0.8465\n",
      "Epoch 9/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3186 - acc: 0.8647 - val_loss: 0.3579 - val_acc: 0.8488\n",
      "Epoch 10/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3180 - acc: 0.8661 - val_loss: 0.3599 - val_acc: 0.8468\n",
      "Epoch 11/35\n",
      "16000/16000 [==============================] - 1s 41us/step - loss: 0.3177 - acc: 0.8650 - val_loss: 0.3598 - val_acc: 0.8482\n",
      "Epoch 12/35\n",
      "16000/16000 [==============================] - 1s 37us/step - loss: 0.3162 - acc: 0.8676 - val_loss: 0.3612 - val_acc: 0.8485\n",
      "Epoch 13/35\n",
      "16000/16000 [==============================] - 1s 37us/step - loss: 0.3178 - acc: 0.8674 - val_loss: 0.3612 - val_acc: 0.8455\n",
      "Epoch 14/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3171 - acc: 0.8681 - val_loss: 0.3580 - val_acc: 0.8458\n",
      "Epoch 15/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3139 - acc: 0.8678 - val_loss: 0.3618 - val_acc: 0.8495\n",
      "Epoch 16/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3146 - acc: 0.8681 - val_loss: 0.3608 - val_acc: 0.8502\n",
      "Epoch 17/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3130 - acc: 0.8686 - val_loss: 0.3611 - val_acc: 0.8435\n",
      "Epoch 18/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3129 - acc: 0.8699 - val_loss: 0.3620 - val_acc: 0.8450\n",
      "Epoch 19/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3127 - acc: 0.8693 - val_loss: 0.3610 - val_acc: 0.8442\n",
      "Epoch 20/35\n",
      "16000/16000 [==============================] - 1s 42us/step - loss: 0.3114 - acc: 0.8693 - val_loss: 0.3610 - val_acc: 0.8427\n",
      "Epoch 21/35\n",
      "16000/16000 [==============================] - 1s 41us/step - loss: 0.3133 - acc: 0.8703 - val_loss: 0.3652 - val_acc: 0.8417\n",
      "Epoch 22/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3125 - acc: 0.8689 - val_loss: 0.3623 - val_acc: 0.8470\n",
      "Epoch 23/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3119 - acc: 0.8707 - val_loss: 0.3678 - val_acc: 0.8452\n",
      "Epoch 24/35\n",
      "16000/16000 [==============================] - 1s 41us/step - loss: 0.3119 - acc: 0.8689 - val_loss: 0.3613 - val_acc: 0.8462\n",
      "Epoch 25/35\n",
      "16000/16000 [==============================] - 1s 41us/step - loss: 0.3104 - acc: 0.8723 - val_loss: 0.3588 - val_acc: 0.8442\n",
      "Epoch 26/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3103 - acc: 0.8702 - val_loss: 0.3631 - val_acc: 0.8405\n",
      "Epoch 27/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3113 - acc: 0.8694 - val_loss: 0.3626 - val_acc: 0.8427\n",
      "Epoch 28/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3115 - acc: 0.8702 - val_loss: 0.3596 - val_acc: 0.8472\n",
      "Epoch 29/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3115 - acc: 0.8699 - val_loss: 0.3602 - val_acc: 0.8460\n",
      "Epoch 30/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3098 - acc: 0.8716 - val_loss: 0.3633 - val_acc: 0.8468\n",
      "Epoch 31/35\n",
      "16000/16000 [==============================] - 1s 41us/step - loss: 0.3087 - acc: 0.8709 - val_loss: 0.3669 - val_acc: 0.8472\n",
      "Epoch 32/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3097 - acc: 0.8709 - val_loss: 0.3632 - val_acc: 0.8490\n",
      "Epoch 33/35\n",
      "16000/16000 [==============================] - 1s 39us/step - loss: 0.3096 - acc: 0.8716 - val_loss: 0.3629 - val_acc: 0.8470\n",
      "Epoch 34/35\n",
      "16000/16000 [==============================] - 1s 38us/step - loss: 0.3093 - acc: 0.8701 - val_loss: 0.3678 - val_acc: 0.8512\n",
      "Epoch 35/35\n",
      "16000/16000 [==============================] - 1s 40us/step - loss: 0.3095 - acc: 0.8724 - val_loss: 0.3643 - val_acc: 0.8395\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "keras.layers.BatchNormalization(axis=-1, \n",
    "momentum=0.99, epsilon=0.001, center=True, \n",
    "scale=True, beta_initializer='zeros', \n",
    "gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "moving_variance_initializer='ones', beta_regularizer=None, \n",
    "gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "\n",
    "model.add(Dense(60, input_dim=1000))\n",
    "keras.layers.LeakyReLU(alpha=0.3)\n",
    "#model.add(Activation('leaky_relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(60)) \n",
    "#model.add(Activation('leaky_relu'))\n",
    "keras.layers.LeakyReLU(alpha=0.3)\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(trainX, trainY, batch_size=64, nb_epoch=35, validation_data=(validX, validY),\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1230625"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(trainX)\n",
    "predictedY = []\n",
    "\n",
    "trainY_new = []\n",
    "for i in range(0, len(trainY)):\n",
    "    if trainY[i][0] == 1:\n",
    "        trainY_new.append(0)\n",
    "    else:\n",
    "        trainY_new.append(1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(prediction)):\n",
    "    if abs(prediction[i][0]-1) < abs(prediction[i][1]-1):\n",
    "        predictedY.append(0)\n",
    "    else:\n",
    "        predictedY.append(1)\n",
    "        \n",
    "class_error(predictedY, trainY_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1605"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_V = model.predict(validX)\n",
    "predictedY_V= []\n",
    "\n",
    "validY_new = []\n",
    "for i in range(0, len(validY)):\n",
    "    if validY[i][0] == 1:\n",
    "        validY_new.append(0)\n",
    "    else:\n",
    "        validY_new.append(1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(prediction_V)):\n",
    "    if abs(prediction_V[i][0]-1) < abs(prediction_V[i][1]-1):\n",
    "        predictedY_V.append(0)\n",
    "    else:\n",
    "        predictedY_V.append(1)\n",
    "        \n",
    "class_error(predictedY_V, validY_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(testX)\n",
    "predictions_arr = []\n",
    "\n",
    "for i in range(0, len(test_predictions)):\n",
    "    if abs(test_predictions[i][0]-1) < abs(test_predictions[i][1]-1):\n",
    "        predictions_arr.append(0)\n",
    "    else:\n",
    "        predictions_arr.append(1)\n",
    "    \n",
    "writeToText(predictions_arr, 'basicNeuralNet2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for writing to the text file for the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToText(predictions, filename):\n",
    "    array = [[\"Id\",\"Prediction\"]]\n",
    "    index = 1\n",
    "    for i in range(len(predictions)):\n",
    "        predict = predictions[index - 1]\n",
    "        array.append([index, int(predictions[index - 1])])\n",
    "        index += 1\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='|')\n",
    "    writer.writerows(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to convert the set into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(X, labels):\n",
    "    documents = []\n",
    "\n",
    "    for element in X:\n",
    "        temp = ''\n",
    "        for i in range(len(element)):\n",
    "            word = labels[i + 1] + ' '\n",
    "            new_word = word * element[i]\n",
    "            temp += (new_word)\n",
    "        documents.append(temp)\n",
    "        \n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "    return sklearn_tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_dim=1000))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(1000)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "inputX = TFIDF(train_X2, train_labels)\n",
    "print(inputX)\n",
    "print(len(inputX))\n",
    "\n",
    "fit = model.fit(inputX, trainY, batch_size=64, nb_epoch=15, validation_data=(validX, validY),\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "prediction = model.predict(trainX)\n",
    "predictedY = []\n",
    "\n",
    "trainY_new = []\n",
    "for i in range(0, len(trainY)):\n",
    "    if trainY[i][0] == 1:\n",
    "        trainY_new.append(0)\n",
    "    else:\n",
    "        trainY_new.append(1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(prediction)):\n",
    "    if abs(prediction[i][0]-1) < abs(prediction[i][1]-1):\n",
    "        predictedY.append(0)\n",
    "    else:\n",
    "        predictedY.append(1)\n",
    "        \n",
    "class_error(predictedY, trainY_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
