{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tflearn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "import sklearn.linear_model as model\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Michelle's method for importing data since I can't do it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX =  (20000, 1000)\n",
      "trainY =  (20000,)\n",
      "testX =  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('training_data.txt',dtype='str')\n",
    "test_data = np.genfromtxt('test_data.txt',dtype='str')\n",
    "#train_data = np.loadtxt('training_data.txt',  delimiter=' ')\n",
    "#test_data = np.loadtxt('test_data.txt')\n",
    "train_labels = train_data[0, :]\n",
    "train_stars = train_data[1:, 0]\n",
    "train_reviews = train_data[1:, 1:]\n",
    "\n",
    "test_labels = test_data[0, :]\n",
    "#test_stars = train_data[1:, 0]\n",
    "test_reviews = test_data[1:, 0:]\n",
    "\n",
    "trainX = train_reviews\n",
    "trainY = train_stars\n",
    "testX = test_reviews\n",
    "#testY = test_stars\n",
    "\n",
    "print(\"trainX = \", trainX.shape)\n",
    "print(\"trainY = \", trainY.shape)\n",
    "print(\"testX = \", testX.shape)\n",
    "\n",
    "\n",
    "for i in range(0, len(trainX)):\n",
    "    for j in range(0, len(trainX[0])):\n",
    "        trainX[i][j] = int(trainX[i][j])\n",
    "\n",
    "for i in range(0, len(testX)):\n",
    "    for j in range(0, len(testX[0])):\n",
    "        testX[i][j] = int(testX[i][j])\n",
    "\n",
    "for i in range(0, len(trainY)):\n",
    "        trainY[i] = int(trainY[i])\n",
    "#for i in range(0, len(trainY)):\n",
    "        #testY[i] = int(testY[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for classification error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_error(y, y_pred):\n",
    "    misclassified = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            misclassified += 1\n",
    "    return float(misclassified) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into 80% training set and 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rows = []\n",
    "for i in range(len(trainY)):\n",
    "    num_rows.append(i)\n",
    "    \n",
    "np.random.shuffle(num_rows)\n",
    "train_indices = num_rows[:16000]\n",
    "valid_indices = num_rows[16000:]\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "valid_X = []\n",
    "valid_y = []\n",
    "\n",
    "for index in train_indices:\n",
    "    train_X.append(trainX[index])\n",
    "    train_y.append(trainY[index])\n",
    "    \n",
    "for index in valid_indices:\n",
    "    valid_X.append(trainX[index])\n",
    "    valid_y.append(trainY[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Michelle's code, it doesn't actually convert the array contents into integers because she tried to do it in place. This code does it correctly out of place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X2 = []\n",
    "for row in train_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    train_X2.append(temp)\n",
    "\n",
    "valid_X2 = []\n",
    "for row in valid_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    valid_X2.append(temp)\n",
    "    \n",
    "valid_y2 = []\n",
    "for row in valid_y:\n",
    "    valid_y2.append(int(row))\n",
    "    \n",
    "train_y2 = []\n",
    "for row in train_y:\n",
    "    train_y2.append(int(row))\n",
    "    \n",
    "test_X2 = []\n",
    "for row in testX:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    test_X2.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything back to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.array(train_X2)\n",
    "validX = np.array(valid_X2)\n",
    "trainY = np.array(train_y2)\n",
    "validY = np.array(valid_y2)\n",
    "testX = np.array(test_X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainY_1D = []\n",
    "#for i in range(0, len(trainY)):\n",
    "    #if trainY[i][0]==1:\n",
    "        #trainY_1D.append(0)\n",
    "    #else:\n",
    "        #trainY_1D.append(1)\n",
    "        \n",
    "#trainY = trainY_1D\n",
    "\n",
    "#print(trainY_1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainY = to_categorical(trainY, 2)\n",
    "\n",
    "net = tflearn.input_data([None, 1000])\n",
    "print(\"made it here 1\")\n",
    "net = tflearn.embedding(net, input_dim=20000, output_dim=128)\n",
    "print(\"made it here 2\")\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "print(\"made it here 3\")\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "print(\"made it here 4\")\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')\n",
    "print(\"made it here 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = tflearn.DNN(net, clip_gradients=0., tensorboard_verbose=2)\n",
    "print(\"made it here 6\")\n",
    "model.fit(trainX, trainY, validation_set =(validX, validY), show_metric=True, batch_size=64)\n",
    "print(\"made it here 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_X2, train_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(logistic.predict(train_X2), train_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(logistic.predict(valid_X2), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression works better than a random forest. Now, we want to combine the training and validation sets again so that we can train our model on all the data to prevent overfitting. We will then use this to predict on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = np.concatenate((train_X2, valid_X2))\n",
    "all_y = np.concatenate((train_y2, valid_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(all_X, all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for writing to the text file for the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToText(predictions, filename):\n",
    "    array = [[\"Id\",\"Prediction\"]]\n",
    "    index = 1\n",
    "    for i in range(len(predictions)):\n",
    "        predict = predictions[index - 1]\n",
    "        array.append([index, int(predictions[index - 1])])\n",
    "        index += 1\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='|')\n",
    "    writer.writerows(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToText(logistic.predict(test_X2), 'logistic.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to convert the set into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(X, labels):\n",
    "    documents = []\n",
    "\n",
    "    for element in X:\n",
    "        temp = ''\n",
    "        for i in range(len(element)):\n",
    "            word = labels[i + 1] + ' '\n",
    "            new_word = word * element[i]\n",
    "            temp += (new_word)\n",
    "        documents.append(temp)\n",
    "        \n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "    return sklearn_tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(logistic.predict(TFIDF(valid_X2, train_labels)), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a lower classification error than the basic regression. We will now undergo the same procedure as in the last section to get the test predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(TFIDF(all_X, train_labels), all_y)\n",
    "writeToText(logistic.predict(TFIDF(test_X2, train_labels)), 'logisticTFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='hinge', penalty='l2', random_state=42).fit(train_X2, train_y2)\n",
    "parameters = {'alpha': (1e-5, 1e-6)}\n",
    "gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=50).fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(nb.predict(TFIDF(valid_X2, train_labels)), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(sgd.predict(TFIDF(valid_X2, train_labels)), valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)\n",
    "sgd.fit(TFIDF(all_X, train_labels), all_y)\n",
    "writeToText(sgd.predict(TFIDF(test_X2, train_labels)), 'SGD_TFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Blend the based on equal weights for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_TFIDF = TFIDF(valid_X2, train_labels)\n",
    "train_TFIDF = TFIDF(train_X2, train_labels)\n",
    "all_TFIDF = TFIDF(all_X, train_labels)\n",
    "test_TFIDF = TFIDF(test_X2, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(train_TFIDF, train_y2)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(train_TFIDF, train_y2)\n",
    "\n",
    "pred1 = logistic.predict(valid_TFIDF)\n",
    "pred2 = sgd.predict(valid_TFIDF)\n",
    "pred3 = nb.predict(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(pred1)):\n",
    "    \n",
    "    if pred1[i] == pred2[i]:\n",
    "        pred.append(pred1[i])\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        pred.append(pred1[i])\n",
    "    else:\n",
    "        pred.append(pred2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(pred, valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(all_TFIDF, all_y)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(all_TFIDF, all_y)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(all_TFIDF, all_y)\n",
    "\n",
    "pred1 = logistic.predict(test_TFIDF)\n",
    "pred2 = sgd.predict(test_TFIDF)\n",
    "pred3 = nb.predict(test_TFIDF)\n",
    "\n",
    "pred = []\n",
    "for i in range(len(pred1)):\n",
    "    \n",
    "    if pred1[i] == pred2[i]:\n",
    "        pred.append(pred1[i])\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        pred.append(pred1[i])\n",
    "    else:\n",
    "        pred.append(pred2[i])\n",
    "\n",
    "writeToText(pred, 'Blend_TFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we need binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X2[train_X2 > 1] = 1\n",
    "valid_X2[valid_X2 > 1] = 1\n",
    "all_X[all_X > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB(alpha=1.0e-10)\n",
    "bnb.fit(train_X2, train_y2)\n",
    "class_error(bnb.predict(valid_X2), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model outputs can output a probability rather than a 0 or 1. We should use these probabilities when classifying the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(train_TFIDF, train_y2)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(train_TFIDF, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_proba(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.decision_function(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.decision_function(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgd and logistic have decision functions, whereas bayes has a probability. Since sgd and logistic are better models, we will look at when the magnitude of their confidence scores is greater than 1. If they both have magnitudes greater than 1 but opposite signs, or if the magnitude for both is less than 1, we get the majority vote from all 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tieBreak(i, pred1, pred2, pred3):\n",
    "    winner = 0\n",
    "    if pred1[i] == pred2[i]:\n",
    "        winner = pred1[i]\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        winner = pred1[i]\n",
    "    else:\n",
    "        winner = pred2[i]\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_confidence = sgd.decision_function(valid_TFIDF)\n",
    "log_confidence = logistic.decision_function(valid_TFIDF)\n",
    "pred1 = sgd.predict(valid_TFIDF)\n",
    "pred2 = logistic.predict(valid_TFIDF)\n",
    "pred3 = nb.predict(valid_TFIDF)\n",
    "\n",
    "pred_blend = []\n",
    "for i in range(len(pred1)):\n",
    "    if abs(sgd_confidence[i]) > 1 and abs(log_confidence[i]) < 1:\n",
    "        pred_blend.append(pred1[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(sgd_confidence[i]) < 1:\n",
    "        pred_blend.append(pred2[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(sgd_confidence[i]) > 1:\n",
    "        if (log_confidence[i] > 0 and sgd_confidence[i] > 0) or (log_confidence[i] < 0 and sgd_confidence[i] < 0):\n",
    "            pred_blend.append(pred1[i])\n",
    "        else:\n",
    "            pred_blend.append(tieBreak(i, pred1, pred2, pred3))\n",
    "    else:\n",
    "        pred_blend.append(tieBreak(i, pred1, pred2, pred3))\n",
    "        print(sgd_confidence[i])\n",
    "        print(log_confidence[i])\n",
    "        print(valid_y2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_error(pred_blend, valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
