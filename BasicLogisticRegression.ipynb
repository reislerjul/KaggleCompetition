{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "import sklearn.linear_model as model\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Michelle's method for importing data since I can't do it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX =  (20000, 1000)\n",
      "trainY =  (20000,)\n",
      "testX =  (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt('Data/training_data.txt',dtype='str')\n",
    "test_data = np.genfromtxt('Data/test_data.txt',dtype='str')\n",
    "#train_data = np.loadtxt('training_data.txt',  delimiter=' ')\n",
    "#test_data = np.loadtxt('test_data.txt')\n",
    "train_labels = train_data[0, :]\n",
    "train_stars = train_data[1:, 0]\n",
    "train_reviews = train_data[1:, 1:]\n",
    "\n",
    "test_labels = test_data[0, :]\n",
    "#test_stars = train_data[1:, 0]\n",
    "test_reviews = test_data[1:, 0:]\n",
    "\n",
    "trainX = train_reviews\n",
    "trainY = train_stars\n",
    "testX = test_reviews\n",
    "#testY = test_stars\n",
    "\n",
    "print(\"trainX = \", trainX.shape)\n",
    "print(\"trainY = \", trainY.shape)\n",
    "print(\"testX = \", testX.shape)\n",
    "\n",
    "\n",
    "for i in range(0, len(trainX)):\n",
    "    for j in range(0, len(trainX[0])):\n",
    "        trainX[i][j] = int(trainX[i][j])\n",
    "\n",
    "for i in range(0, len(testX)):\n",
    "    for j in range(0, len(testX[0])):\n",
    "        testX[i][j] = int(testX[i][j])\n",
    "\n",
    "for i in range(0, len(trainY)):\n",
    "        trainY[i] = int(trainY[i])\n",
    "#for i in range(0, len(trainY)):\n",
    "        #testY[i] = int(testY[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for classification error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_error(y, y_pred):\n",
    "    misclassified = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != y_pred[i]:\n",
    "            misclassified += 1\n",
    "    return float(misclassified) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into 80% training set and 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rows = []\n",
    "for i in range(len(trainY)):\n",
    "    num_rows.append(i)\n",
    "    \n",
    "np.random.shuffle(num_rows)\n",
    "train_indices = num_rows[:16000]\n",
    "valid_indices = num_rows[16000:]\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "valid_X = []\n",
    "valid_y = []\n",
    "\n",
    "for index in train_indices:\n",
    "    train_X.append(trainX[index])\n",
    "    train_y.append(trainY[index])\n",
    "    \n",
    "for index in valid_indices:\n",
    "    valid_X.append(trainX[index])\n",
    "    valid_y.append(trainY[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Michelle's code, it doesn't actually convert the array contents into integers because she tried to do it in place. This code does it correctly out of place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X2 = []\n",
    "for row in train_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    train_X2.append(temp)\n",
    "\n",
    "valid_X2 = []\n",
    "for row in valid_X:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    valid_X2.append(temp)\n",
    "    \n",
    "valid_y2 = []\n",
    "for row in valid_y:\n",
    "    valid_y2.append(int(row))\n",
    "    \n",
    "train_y2 = []\n",
    "for row in train_y:\n",
    "    train_y2.append(int(row))\n",
    "    \n",
    "test_X2 = []\n",
    "for row in testX:\n",
    "    temp = []\n",
    "    for element in row:\n",
    "        temp.append(int(element))\n",
    "    test_X2.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything back to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X2 = np.array(train_X2)\n",
    "valid_X2 = np.array(valid_X2)\n",
    "train_y2 = np.array(train_y2)\n",
    "valid_y2 = np.array(valid_y2)\n",
    "test_X2 = np.array(test_X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_X2, train_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.118625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(logistic.predict(train_X2), train_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification error on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16025"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(logistic.predict(valid_X2), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression works better than a random forest. Now, we want to combine the training and validation sets again so that we can train our model on all the data to prevent overfitting. We will then use this to predict on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = np.concatenate((train_X2, valid_X2))\n",
    "all_y = np.concatenate((train_y2, valid_y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(all_X, all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for writing to the text file for the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToText(predictions, filename):\n",
    "    array = [[\"Id\",\"Prediction\"]]\n",
    "    index = 1\n",
    "    for i in range(len(predictions)):\n",
    "        predict = predictions[index - 1]\n",
    "        array.append([index, int(predictions[index - 1])])\n",
    "        index += 1\n",
    "    f = open(filename, 'w')\n",
    "    writer = csv.writer(f, delimiter=',', quotechar='|')\n",
    "    writer.writerows(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToText(logistic.predict(test_X2), 'logistic.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to convert the set into a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(X, labels):\n",
    "    documents = []\n",
    "\n",
    "    for element in X:\n",
    "        temp = ''\n",
    "        for i in range(len(element)):\n",
    "            word = labels[i + 1] + ' '\n",
    "            new_word = word * element[i]\n",
    "            temp += (new_word)\n",
    "        documents.append(temp)\n",
    "        \n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "    return sklearn_tfidf.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15375"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(logistic.predict(TFIDF(valid_X2, train_labels)), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a lower classification error than the basic regression. We will now undergo the same procedure as in the last section to get the test predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(TFIDF(all_X, train_labels), all_y)\n",
    "writeToText(logistic.predict(TFIDF(test_X2, train_labels)), 'logisticTFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='hinge', penalty='l2', random_state=42).fit(train_X2, train_y2)\n",
    "parameters = {'alpha': (1e-5, 1e-6)}\n",
    "gs_clf = GridSearchCV(clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-05}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=50).fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(nb.predict(TFIDF(valid_X2, train_labels)), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(TFIDF(train_X2, train_labels), train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15325"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(sgd.predict(valid_TFIDF), valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)\n",
    "sgd.fit(TFIDF(all_X, train_labels), all_y)\n",
    "writeToText(sgd.predict(TFIDF(test_X2, train_labels)), 'SGD_TFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Blend the based on equal weights for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_TFIDF = TFIDF(valid_X2, train_labels)\n",
    "train_TFIDF = TFIDF(train_X2, train_labels)\n",
    "all_TFIDF = TFIDF(all_X, train_labels)\n",
    "test_TFIDF = TFIDF(test_X2, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(train_TFIDF, train_y2)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(train_TFIDF, train_y2)\n",
    "\n",
    "pred1 = logistic.predict(valid_TFIDF)\n",
    "pred2 = sgd.predict(valid_TFIDF)\n",
    "pred3 = nb.predict(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(len(pred1)):\n",
    "    \n",
    "    if pred1[i] == pred2[i]:\n",
    "        pred.append(pred1[i])\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        pred.append(pred1[i])\n",
    "    else:\n",
    "        pred.append(pred2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15025"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(pred, valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(all_TFIDF, all_y)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(all_TFIDF, all_y)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(all_TFIDF, all_y)\n",
    "\n",
    "pred1 = logistic.predict(test_TFIDF)\n",
    "pred2 = sgd.predict(test_TFIDF)\n",
    "pred3 = nb.predict(test_TFIDF)\n",
    "\n",
    "pred = []\n",
    "for i in range(len(pred1)):\n",
    "    \n",
    "    if pred1[i] == pred2[i]:\n",
    "        pred.append(pred1[i])\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        pred.append(pred1[i])\n",
    "    else:\n",
    "        pred.append(pred2[i])\n",
    "\n",
    "writeToText(pred, 'Blend_TFIDF.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we need binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2[train_X2 > 1] = 1\n",
    "valid_X2[valid_X2 > 1] = 1\n",
    "all_X[all_X > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1795"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNB(alpha=1.0e-10)\n",
    "bnb.fit(train_X2, train_y2)\n",
    "class_error(bnb.predict(valid_X2), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model outputs can output a probability rather than a 0 or 1. We should use these probabilities when classifying the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(train_TFIDF, train_y2)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(train_TFIDF, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37797752,  0.62202248],\n",
       "       [ 0.65392058,  0.34607942],\n",
       "       [ 0.48520922,  0.51479078],\n",
       "       ..., \n",
       "       [ 0.60520652,  0.39479348],\n",
       "       [ 0.49716815,  0.50283185],\n",
       "       [ 0.41540171,  0.58459829]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict_proba(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.23658711, -1.94103518,  0.84981656, ..., -1.26886093,\n",
       "       -0.86700456,  1.0053024 ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.decision_function(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.84006047, -2.53061288,  0.62282974, ..., -1.99477366,\n",
       "       -1.02389807,  1.59835106])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.decision_function(valid_TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgd and logistic have decision functions, whereas bayes has a probability. Since sgd and logistic are better models, we will look at when the magnitude of their confidence scores is greater than 1. If they both have magnitudes greater than 1 but opposite signs, or if the magnitude for both is less than 1, we get the majority vote from all 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tieBreak(i, pred1, pred2, pred3):\n",
    "    winner = 0\n",
    "    if pred1[i] == pred2[i]:\n",
    "        winner = pred1[i]\n",
    "    elif pred1[i] == pred3[i]:\n",
    "        winner = pred1[i]\n",
    "    else:\n",
    "        winner = pred2[i]\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_confidence = sgd.decision_function(valid_TFIDF)\n",
    "log_confidence = logistic.decision_function(valid_TFIDF)\n",
    "pred1 = sgd.predict(valid_TFIDF)\n",
    "pred2 = logistic.predict(valid_TFIDF)\n",
    "pred3 = nb.predict(valid_TFIDF)\n",
    "\n",
    "pred_blend = []\n",
    "for i in range(len(pred1)):\n",
    "    if abs(sgd_confidence[i]) > 1 and abs(log_confidence[i]) < 1:\n",
    "        pred_blend.append(pred1[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(sgd_confidence[i]) < 1:\n",
    "        pred_blend.append(pred2[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(sgd_confidence[i]) > 1:\n",
    "        if (log_confidence[i] > 0 and sgd_confidence[i] > 0) or (log_confidence[i] < 0 and sgd_confidence[i] < 0):\n",
    "            pred_blend.append(pred1[i])\n",
    "        else:\n",
    "            pred_blend.append(tieBreak(i, pred1, pred2, pred3))\n",
    "    else:\n",
    "        pred_blend.append(tieBreak(i, pred1, pred2, pred3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15025"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(pred_blend, valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=2, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = LinearSVC(random_state=0, loss='hinge', C=2)\n",
    "lin.fit(train_TFIDF, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15475"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(lin.predict(valid_TFIDF), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=15, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "        tol=0.001)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeClassifier(alpha=15, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "ridge.fit(train_TFIDF, train_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1525"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(ridge.predict(valid_TFIDF), valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Weighted Blends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tieBreak2(i, pred1, pred2, pred3, pred4, pred5):\n",
    "    preds = [pred1, pred2, pred3, pred4, pred5]\n",
    "    zeros = 0\n",
    "    for element in preds:\n",
    "        if element[i] == 0:\n",
    "            zeros += 1\n",
    "    if zeros > 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(train_TFIDF, train_y2)\n",
    "\n",
    "lin = LinearSVC(random_state=0, loss='hinge', C=2)\n",
    "lin.fit(train_TFIDF, train_y2)\n",
    "\n",
    "ridge = RidgeClassifier(alpha=15, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "ridge.fit(train_TFIDF, train_y2)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(train_TFIDF, train_y2)\n",
    "\n",
    "sgd_confidence = sgd.decision_function(valid_TFIDF)\n",
    "log_confidence = logistic.decision_function(valid_TFIDF)\n",
    "lin_confidence = lin.decision_function(valid_TFIDF)\n",
    "ridge_confidence = ridge.decision_function(valid_TFIDF)\n",
    "\n",
    "pred1 = sgd.predict(valid_TFIDF)\n",
    "pred2 = logistic.predict(valid_TFIDF)\n",
    "pred3 = nb.predict(valid_TFIDF)\n",
    "pred4 = lin.predict(valid_TFIDF)\n",
    "pred5 = ridge.predict(valid_TFIDF)\n",
    "\n",
    "pred_blend = []\n",
    "for i in range(len(pred1)):\n",
    "    if abs(lin_confidence[i]) > 1 and abs(log_confidence[i]) < 1:\n",
    "        pred_blend.append(pred1[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(lin_confidence[i]) < 1:\n",
    "        pred_blend.append(pred2[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(lin_confidence[i]) > 1:\n",
    "        if (log_confidence[i] > 0 and lin_confidence[i] > 0) or (log_confidence[i] < 0 and lin_confidence[i] < 0):\n",
    "            pred_blend.append(pred1[i])\n",
    "        else:\n",
    "            pred_blend.append(tieBreak(i, pred4, pred2, pred3))\n",
    "    else:\n",
    "        pred_blend.append(tieBreak(i, pred4, pred2, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.149"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(pred_blend, valid_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a combination of linearSVC and LogisticRegression with Naive Bayes as the tie break works the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic = model.LogisticRegression()\n",
    "logistic.fit(all_TFIDF, all_y)\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, random_state=42)\n",
    "sgd.fit(all_TFIDF, all_y)\n",
    "\n",
    "lin = LinearSVC(random_state=0, loss='hinge', C=2)\n",
    "lin.fit(all_TFIDF, all_y)\n",
    "\n",
    "ridge = RidgeClassifier(alpha=15, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "ridge.fit(all_TFIDF, all_y)\n",
    "\n",
    "nb = MultinomialNB(alpha=50).fit(all_TFIDF, all_y)\n",
    "\n",
    "sgd_confidence = sgd.decision_function(test_TFIDF)\n",
    "log_confidence = logistic.decision_function(test_TFIDF)\n",
    "lin_confidence = lin.decision_function(test_TFIDF)\n",
    "ridge_confidence = ridge.decision_function(test_TFIDF)\n",
    "\n",
    "pred1 = sgd.predict(test_TFIDF)\n",
    "pred2 = logistic.predict(test_TFIDF)\n",
    "pred3 = nb.predict(test_TFIDF)\n",
    "pred4 = lin.predict(test_TFIDF)\n",
    "pred5 = ridge.predict(test_TFIDF)\n",
    "\n",
    "pred_blend = []\n",
    "for i in range(len(pred1)):\n",
    "    if abs(lin_confidence[i]) > 1 and abs(log_confidence[i]) < 1:\n",
    "        pred_blend.append(pred1[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(lin_confidence[i]) < 1:\n",
    "        pred_blend.append(pred2[i])\n",
    "    elif abs(log_confidence[i]) > 1 and abs(lin_confidence[i]) > 1:\n",
    "        if (log_confidence[i] > 0 and lin_confidence[i] > 0) or (log_confidence[i] < 0 and lin_confidence[i] < 0):\n",
    "            pred_blend.append(pred1[i])\n",
    "        else:\n",
    "            pred_blend.append(tieBreak(i, pred4, pred2, pred3))\n",
    "    else:\n",
    "        pred_blend.append(tieBreak(i, pred4, pred2, pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToText(pred_blend, 'Blend_TFIDF_Log_Lin.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1000, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=1000)\n",
    "neigh.fit(train_TFIDF, train_y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.181"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_error(neigh.predict(valid_TFIDF), valid_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
